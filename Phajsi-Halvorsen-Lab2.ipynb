{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "397a8eb9",
   "metadata": {},
   "source": [
    "# Lab 2 - Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d50aee",
   "metadata": {},
   "source": [
    "## 1 – Ambiguity\n",
    "Use the Brown Corpus and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "27e63dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf044a8",
   "metadata": {},
   "source": [
    "**(a) Print the 5 most frequent tags in the corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e76a1084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The five most common tagged words are:\n",
      "(('the', 'DET'), 69968)\n",
      "(('of', 'ADP'), 36410)\n",
      "(('and', 'CONJ'), 28850)\n",
      "(('a', 'DET'), 23070)\n",
      "(('in', 'ADP'), 20866)\n"
     ]
    }
   ],
   "source": [
    "#computing the frequency distribution of the tagged words, filtered by alphanumeric characters\n",
    "brown_tagged = brown.tagged_words(tagset='universal')\n",
    "brown_tagged = [(word.lower(), tag) for (word,tag) in brown_tagged if word.isalnum()]\n",
    "\n",
    "brown_fdist = nltk.FreqDist(brown_tagged)\n",
    "five_most_common = brown_fdist.most_common(5)\n",
    "\n",
    "print(\"The five most common tagged words are:\")\n",
    "for word in five_most_common:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b49e448",
   "metadata": {},
   "source": [
    "**(b) How many words are ambiguous, in the sense that they appear with more than two tags?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "38719ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272  words are ambiguous\n"
     ]
    }
   ],
   "source": [
    "brown_cond_freq = nltk.ConditionalFreqDist(brown_tagged)\n",
    "ambiguous_words = []\n",
    "\n",
    "for word in brown_cond_freq.conditions():\n",
    "   if(len(brown_cond_freq[word]) > 2):\n",
    "      ambiguous_words.append(word)\n",
    "\n",
    "print(len(ambiguous_words),' words are ambiguous')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3ef592",
   "metadata": {},
   "source": [
    "**(c) Print the percentage of ambiguous words in the corpus1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "14e26f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272 988331\n",
      "0.0275 % of the words in Brown corpus are ambiguous\n"
     ]
    }
   ],
   "source": [
    "print(len(ambiguous_words), len(brown_tagged))\n",
    "print(\"{:.4f}\".format(len(ambiguous_words)*100/len(brown_tagged)), \"% of the words in Brown corpus are ambiguous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed9720",
   "metadata": {},
   "source": [
    "**(d) Find the top 5 words (longer than 4 characters) with the highest number of distinct tags. Select one\n",
    "of them and print out a sentence with the word in its different forms.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5fb6ea1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 words with the highest number of tags are:\n",
      "round ['NOUN', 'VERB', 'ADJ', 'ADV', 'ADP']\n",
      "present ['ADJ', 'ADV', 'NOUN', 'VERB']\n",
      "about ['ADP', 'ADV', 'PRT', 'X']\n",
      "still ['ADV', 'VERB', 'ADJ', 'NOUN']\n",
      "close ['NOUN', 'ADJ', 'ADV', 'VERB']\n"
     ]
    }
   ],
   "source": [
    "words_longer_4_char = {}\n",
    "for word in brown_cond_freq.conditions():\n",
    "   if(len(word) > 4):\n",
    "      tags = brown_cond_freq[word].keys()\n",
    "      words_longer_4_char[word] = list(tags)\n",
    "\n",
    "print(\"The top 5 words with the highest number of tags are:\")\n",
    "for word in sorted(words_longer_4_char, key=lambda word: len(words_longer_4_char[word]), reverse = True)[:5]:\n",
    "    print(word, words_longer_4_char[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f01e457b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Present' as ADJ\n",
      "[('The', 'DET'), (\"mayor's\", 'NOUN'), ('present', 'ADJ'), ('term', 'NOUN'), ('of', 'ADP'), ('office', 'NOUN'), ('expires', 'VERB'), ('Jan.', 'NOUN'), ('1', 'NUM'), ('.', '.')] \n",
      "\n",
      "'Present' as ADV\n",
      "[('In', 'ADP'), ('1960', 'NUM'), ('more', 'ADJ'), ('than', 'ADP'), ('6,000', 'NUM'), ('Communist', 'NOUN'), ('technicians', 'NOUN'), ('were', 'VERB'), ('present', 'ADV'), ('in', 'ADP'), ('those', 'DET'), ('countries', 'NOUN'), ('.', '.')] \n",
      "\n",
      "'Present' as NOUN\n",
      "[('But', 'CONJ'), ('an', 'DET'), ('official', 'ADJ'), ('statement', 'NOUN'), ('adopted', 'VERB'), ('by', 'ADP'), ('the', 'DET'), ('33-man', 'ADJ'), ('Emory', 'NOUN'), ('board', 'NOUN'), ('at', 'ADP'), ('its', 'DET'), ('annual', 'ADJ'), ('meeting', 'NOUN'), ('Friday', 'NOUN'), ('noted', 'VERB'), ('that', 'ADP'), ('state', 'NOUN'), ('taxing', 'VERB'), ('requirements', 'NOUN'), ('at', 'ADP'), ('present', 'NOUN'), ('are', 'VERB'), ('a', 'DET'), ('roadblock', 'NOUN'), ('to', 'ADP'), ('accepting', 'VERB'), ('Negroes', 'NOUN'), ('.', '.')] \n",
      "\n",
      "'Present' as VERB\n",
      "[('``', '.'), ('Le', 'X'), ('Theatre', 'X'), (\"D'Art\", 'X'), ('Du', 'X'), ('Ballet', 'X'), (\"''\", '.'), (',', '.'), ('of', 'ADP'), ('Monte', 'NOUN'), ('Carlo', 'NOUN'), (',', '.'), ('will', 'VERB'), ('present', 'VERB'), ('a', 'DET'), ('program', 'NOUN'), ('of', 'ADP'), ('four', 'NUM'), ('ballets', 'NOUN'), ('including', 'ADP'), ('``', '.'), ('Francesca', 'NOUN'), ('Da', 'NOUN'), ('Rimini', 'NOUN'), (\"''\", '.'), ('.', '.')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# finding sentences for all POS for \"present\"\n",
    "present_POS = ['ADJ', 'ADV', 'NOUN', 'VERB']\n",
    "\n",
    "for sentence in brown.tagged_sents(tagset='universal'):\n",
    "    for (word, tag) in sentence:\n",
    "        if(word == 'present' and tag in present_POS):\n",
    "            print(\"'Present' as \" + tag)\n",
    "            print(sentence, '\\n')\n",
    "            present_POS.remove(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e61ef58",
   "metadata": {},
   "source": [
    "**(e) Discuss and think about how you would attack the problem of resolving ambiguous words for a\n",
    "predictive smartphone keyboard**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00128fe",
   "metadata": {},
   "source": [
    "I would attack the problem by basing the predictivity algorithm on bigrams. In this way the POS of the last written word has an impact on the probability of the word being predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c5f90",
   "metadata": {},
   "source": [
    "## 2 – Training a tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963f53ed",
   "metadata": {},
   "source": [
    "Explore the performance of a tagger using the Brown Corpus and NPS Chat Corpus as data sources, with different ratios of train/test data. Use the following ratios:\n",
    "\n",
    "• Brown 90%/NPS 10%\n",
    "• Brown 50%/NPS 50%\n",
    "• NPS 90%/Brown 10%\n",
    "• NPS 50%/Brown 50%\n",
    "\n",
    "Create the taggers listed below and comment your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef7816",
   "metadata": {},
   "source": [
    "**(a) Create a DefaultTagger using the most common tag in each corpus as the default tag.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "74f4211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown, nps_chat\n",
    "from nltk import DefaultTagger\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from nltk.tag import DefaultTagger, UnigramTagger, BigramTagger\n",
    "\n",
    "#nltk.download('nps_chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a53caa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browns most frequent tag is:  NN\n",
      "NPS most frequent tag is:  UH\n"
     ]
    }
   ],
   "source": [
    "# from https://www.nltk.org/book/ch05.html\n",
    "\n",
    "tags_brown = [tag for (word, tag) in brown.tagged_words()]\n",
    "print('Browns most frequent tag is: ', nltk.FreqDist(tags_brown).max())\n",
    "\n",
    "tags_nps = [tag for (word, tag) in nps_chat.tagged_words()]\n",
    "print('NPS most frequent tag is: ',nltk.FreqDist(tags_nps).max())\n",
    "\n",
    "default_tagger_brown = DefaultTagger('NN')\n",
    "default_tagger_nps = DefaultTagger('UH')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84b7d1c",
   "metadata": {},
   "source": [
    "**(b) Create a combined tagger with the RegEx tagger (see Ch. 5, sec. 4.2) with an initial backoff using\n",
    "the most common default tag. Then, use n-gram taggers as backoff taggers (e.g., UnigramTagger, BigramTagger, TrigramTagger). The ordering is up to you, but justify your choice. Calculate the accuracy of each of the four train/test permutations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "de9b7872",
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_tagged = nltk.corpus.brown.tagged_sents()\n",
    "brown_train_50, brown_test_50 = split(brown_tagged, test_size=0.5)\n",
    "\n",
    "nps_tagged = nltk.corpus.nps_chat.tagged_posts()\n",
    "nps_train_50, nps_test_50 = split(nps_tagged, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1e140cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a combined tagger with the RegEx tagger with an initial backoff using the most common default tag. \n",
    "patterns = [\n",
    "    (r'.*ing$', 'VBG'),                # gerunds\n",
    "    (r'.*ed$', 'VBD'),                 # simple past\n",
    "    (r'.*es$', 'VBZ'),                 # 3rd singular present\n",
    "    (r'.*ould$', 'MD'),                # modals\n",
    "    (r'.*\\'s$', 'NN$'),                # possessive nouns\n",
    "    (r'.*s$', 'NNS'),                  # plural nouns\n",
    "    (r'^-?[0-9]+(\\.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "]\n",
    "regexp_tagger_brown_backoff = nltk.RegexpTagger(patterns, backoff = default_tagger_brown)\n",
    "regexp_tagger_nps_backoff = nltk.RegexpTagger(patterns, backoff = default_tagger_nps)\n",
    "\n",
    "#Creating bigram taggers with unigram backoff taggers\n",
    "\n",
    "combined_brown_unigram_tagger_regex_backoff = UnigramTagger(train = brown_train_50, backoff = regexp_tagger_brown_backoff)\n",
    "combined_brown_bigram_tagger_unigram_backoff = BigramTagger(train = brown_train_50, backoff = combined_brown_unigram_tagger_regex_backoff)\n",
    "\n",
    "combined_nps_unigram_tagger_regex_backoff = UnigramTagger(train = nps_train_50, backoff = regexp_tagger_nps_backoff)\n",
    "combined_nps_bigram_tagger_unigram_backoff = BigramTagger(train = nps_train_50, backoff = combined_nps_unigram_tagger_regex_backoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "348ad306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Brown train data on Brown test data 0.918\n",
      "Accuracy of Brown train data on NPS test data 0.354\n",
      "Accuracy of NPS train data on NPS test data 0.866\n",
      "Accuracy of NPS train data on Brown test data 0.427\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate the accuracy of each of the four train/test permutations.\n",
    "print('Accuracy of Brown train data on Brown test data', \n",
    "    '{:.3f}'.format(combined_brown_bigram_tagger_unigram_backoff.accuracy(brown_test_50)))\n",
    "\n",
    "print('Accuracy of Brown train data on NPS test data', \n",
    "    '{:.3f}'.format(combined_brown_bigram_tagger_unigram_backoff.accuracy(nps_test_50)))\n",
    "    \n",
    "print('Accuracy of NPS train data on NPS test data', \n",
    "    '{:.3f}'.format(combined_nps_bigram_tagger_unigram_backoff.accuracy(nps_test_50)))\n",
    "    \n",
    "print('Accuracy of NPS train data on Brown test data', \n",
    "    '{:.3f}'.format(combined_nps_bigram_tagger_unigram_backoff.accuracy(brown_test_50)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9099cc84",
   "metadata": {},
   "source": [
    "**(c) Select a dataset split of your choice and print a table containing the precision, recall and f-measure\n",
    "for the top 5 most common tags (look up truncate in the documentation) and sort each score by\n",
    "count. Do this for all your chosen variations of backoffs (e.g., DefaultTagger, UnigramTagger and\n",
    "BigramTagger).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2e49acf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram with Unigram backoff, trained on Brown data \n",
      " Tag | Prec.  | Recall | F-measure\n",
      "----+--------+--------+-----------\n",
      " NN | 0.9787 | 0.9858 | 0.9822\n",
      " IN | 0.9373 | 0.9273 | 0.9323\n",
      " AT | 0.9928 | 0.9983 | 0.9955\n",
      " JJ | 0.9528 | 0.9737 | 0.9631\n",
      "  . | 0.9978 | 0.9982 | 0.9980\n",
      "\n",
      "Bigram with Unigram backoff, trained on NPS data \n",
      " Tag | Prec.  | Recall | F-measure\n",
      "----+--------+--------+-----------\n",
      " NN | 0.8305 | 0.2847 | 0.4240\n",
      " IN | 0.8276 | 0.8351 | 0.8313\n",
      " AT | 0.0000 | 0.0000 | 0.0000\n",
      " JJ | 0.5829 | 0.2639 | 0.3634\n",
      "  . | 0.9889 | 0.8590 | 0.9194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Bigram with Unigram backoff, trained on Brown data \\n', combined_brown_bigram_tagger_unigram_backoff.evaluate_per_tag(brown_train_50, truncate=5, sort_by_count=True))\n",
    "print('Bigram with Unigram backoff, trained on NPS data \\n', combined_nps_bigram_tagger_unigram_backoff.evaluate_per_tag(brown_train_50, truncate=5, sort_by_count=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1f692290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram with Regex backoff, trained on Brown data \n",
      " Tag | Prec.  | Recall | F-measure\n",
      "----+--------+--------+-----------\n",
      " NN | 0.9435 | 0.9522 | 0.9478\n",
      " IN | 0.9430 | 0.8873 | 0.9143\n",
      " AT | 0.9846 | 1.0000 | 0.9922\n",
      " JJ | 0.9215 | 0.9554 | 0.9382\n",
      "  . | 0.9901 | 0.9993 | 0.9947\n",
      "\n",
      "Unigram with Regex backoff, trained on NPS data \n",
      " Tag | Prec.  | Recall | F-measure\n",
      "----+--------+--------+-----------\n",
      " NN | 0.8254 | 0.2759 | 0.4136\n",
      " IN | 0.8266 | 0.8434 | 0.8349\n",
      " AT | 0.0000 | 0.0000 | 0.0000\n",
      " JJ | 0.5828 | 0.2676 | 0.3667\n",
      "  . | 0.9891 | 0.9076 | 0.9466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Unigram with Regex backoff, trained on Brown data \\n', combined_brown_unigram_tagger_regex_backoff.evaluate_per_tag(brown_train_50, truncate=5, sort_by_count=True))\n",
    "print('Unigram with Regex backoff, trained on NPS data \\n', combined_nps_unigram_tagger_regex_backoff.evaluate_per_tag(brown_train_50, truncate=5, sort_by_count=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "080d7518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regex with most frquent word in Brown (NN) backoff, trained on NPS data \n",
      " Tag | Prec.  | Recall | F-measure\n",
      "----+--------+--------+-----------\n",
      " NN | 0.1501 | 0.9383 | 0.2588\n",
      " IN | 0.0000 | 0.0000 | 0.0000\n",
      " AT | 0.0000 | 0.0000 | 0.0000\n",
      " JJ | 0.0000 | 0.0000 | 0.0000\n",
      "  . | 0.0000 | 0.0000 | 0.0000\n",
      "\n",
      "Regex with most frquent word in NPS (UH) backoff, trained on NPS data \n",
      " Tag | Prec.  | Recall | F-measure\n",
      "----+--------+--------+-----------\n",
      " NN | 0.0000 | 0.0000 | 0.0000\n",
      " IN | 0.0000 | 0.0000 | 0.0000\n",
      " AT | 0.0000 | 0.0000 | 0.0000\n",
      " JJ | 0.0000 | 0.0000 | 0.0000\n",
      "  . | 0.0000 | 0.0000 | 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Regex with most frquent word in Brown (NN) backoff, trained on NPS data \\n', regexp_tagger_brown_backoff.evaluate_per_tag(brown_train_50, truncate=5, sort_by_count=True))\n",
    "print('Regex with most frquent word in NPS (UH) backoff, trained on NPS data \\n', regexp_tagger_nps_backoff.evaluate_per_tag(brown_train_50, truncate=5, sort_by_count=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b556a15",
   "metadata": {},
   "source": [
    "**(d) Using the Brown Corpus, create a baseline tagger (e.g. Unigram) with a lookup model (see Ch. 5,\n",
    "sec. 4.3). The model should handle the most 200 common words and store the tags. Evaluate the\n",
    "accuracy on the above permutations of train/test data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c670d2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Brown train data 0.5338\n",
      "Evaluating Brown test data 0.5349\n",
      "Evaluating NPS train data 0.1721\n",
      "Evaluating NPS test data 0.1697\n"
     ]
    }
   ],
   "source": [
    "fd = nltk.FreqDist(brown.words())\n",
    "cfd = nltk.ConditionalFreqDist(brown.tagged_words())\n",
    "most_freq_words = fd.most_common(200)\n",
    "likely_tags = dict((word, cfd[word].max()) for (word, _) in most_freq_words)\n",
    "baseline_tagger_most_frequent_words = nltk.UnigramTagger(model=likely_tags)\n",
    "\n",
    "print('Evaluating Brown train data', '{:.4}'.format(baseline_tagger_most_frequent_words.accuracy(brown_train_50)))\n",
    "print('Evaluating Brown test data', '{:.4}'.format(baseline_tagger_most_frequent_words.accuracy(brown_test_50)))\n",
    "print('Evaluating NPS train data', '{:.4}'.format(baseline_tagger_most_frequent_words.accuracy(nps_train_50)))\n",
    "print('Evaluating NPS test data', '{:.4}'.format(baseline_tagger_most_frequent_words.accuracy(nps_test_50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e796b37",
   "metadata": {},
   "source": [
    "**(e) With an arbitrary text from another corpus (or an article you scraped in Lab 1), use the tagger you\n",
    "just created and print a few tagged sentences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "df980d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('On', None), ('the', 'AT'), ('other', 'AP'), ('hand', None), (',', ','), ('the', 'AT'), ('magnitude', None), ('and', 'CC'), ('difficulty', None), ('of', 'IN'), ('the', 'AT'), ('trust', None), ('to', 'TO'), ('which', 'WDT'), ('the', 'AT'), ('voice', None), ('of', 'IN'), ('my', 'PP$'), ('country', None), ('called', None), ('me', 'PPO'), (',', ','), ('being', 'BEG'), ('sufficient', None), ('to', 'TO'), ('awaken', None), ('in', 'IN'), ('the', 'AT'), ('wisest', None), ('and', 'CC'), ('most', 'QL'), ('experienced', None), ('of', 'IN'), ('her', 'PP$'), ('citizens', None), ('a', 'AT'), ('distrustful', None), ('scrutiny', None), ('into', 'IN'), ('his', 'PP$'), ('qualifications', None), (',', ','), ('could', 'MD'), ('not', '*'), ('but', 'CC'), ('overwhelm', None), ('with', 'IN'), ('despondence', None), ('one', 'CD'), ('who', 'WPS'), ('(', '('), ('inheriting', None), ('inferior', None), ('endowments', None), ('from', 'IN'), ('nature', None), ('and', 'CC'), ('unpracticed', None), ('in', 'IN'), ('the', 'AT'), ('duties', None), ('of', 'IN'), ('civil', None), ('administration', None), (')', ')'), ('ought', None), ('to', 'TO'), ('be', 'BE'), ('peculiarly', None), ('conscious', None), ('of', 'IN'), ('his', 'PP$'), ('own', 'JJ'), ('deficiencies', None), ('.', '.')], [('In', 'IN'), ('this', 'DT'), ('conflict', None), ('of', 'IN'), ('emotions', None), ('all', 'ABN'), ('I', 'PPSS'), ('dare', None), ('aver', None), ('is', 'BEZ'), ('that', 'CS'), ('it', 'PPS'), ('has', 'HVZ'), ('been', 'BEN'), ('my', 'PP$'), ('faithful', None), ('study', None), ('to', 'TO'), ('collect', None), ('my', 'PP$'), ('duty', None), ('from', 'IN'), ('a', 'AT'), ('just', 'RB'), ('appreciation', None), ('of', 'IN'), ('every', None), ('circumstance', None), ('by', 'IN'), ('which', 'WDT'), ('it', 'PPS'), ('might', 'MD'), ('be', 'BE'), ('affected', None), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "#with the help of this Q&A in Stackoverflow\n",
    "#https://stackoverflow.com/questions/48768084/nltk-unigramtagger-typeerror-unhashable-type-list\n",
    "\n",
    "from nltk.corpus import inaugural\n",
    "#nltk.download('inaugural')\n",
    "\n",
    "inaugural_text = inaugural.sents()[3:5]\n",
    "inaugural_text = [tuple(sent) for sent in inaugural_text]\n",
    "\n",
    "inaugural_tagged = [baseline_tagger_most_frequent_words.tag(word) for word in inaugural_text]\n",
    "print(inaugural_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b49758",
   "metadata": {},
   "source": [
    "**(f) Experiment with different ratios and using only one dataset with a train/test split. Explain your\n",
    "findings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2c41304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nps_tagged = nltk.corpus.nps_chat.tagged_posts()\n",
    "nps_train_90, nps_test_10 = split(nps_tagged, test_size=0.1)\n",
    "nps_train_80, nps_test_20 = split(nps_tagged, test_size=0.2)\n",
    "nps_train_50, nps_test_50 = split(nps_tagged, test_size=0.5)\n",
    "nps_train_20, nps_test_80 = split(nps_tagged, test_size=0.8)\n",
    "nps_train_10, nps_test_90 = split(nps_tagged, test_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ecae888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_nps90_unigram_tagger_regex_backoff = UnigramTagger(train = nps_train_90, backoff = regexp_tagger_nps_backoff)\n",
    "combined_nps80_unigram_tagger_regex_backoff = UnigramTagger(train = nps_train_80, backoff = regexp_tagger_nps_backoff)\n",
    "combined_nps50_unigram_tagger_regex_backoff = UnigramTagger(train = nps_train_50, backoff = regexp_tagger_nps_backoff)\n",
    "combined_nps20_unigram_tagger_regex_backoff = UnigramTagger(train = nps_train_20, backoff = regexp_tagger_nps_backoff)\n",
    "combined_nps10_unigram_tagger_regex_backoff = UnigramTagger(train = nps_train_10, backoff = regexp_tagger_nps_backoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "df4d6ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nps_train90 = combined_nps90_unigram_tagger_regex_backoff.accuracy(nps_test_10)\n",
    "nps_train80 = combined_nps80_unigram_tagger_regex_backoff.accuracy(nps_test_20)\n",
    "nps_train50 = combined_nps50_unigram_tagger_regex_backoff.accuracy(nps_test_50)\n",
    "nps_train20 = combined_nps20_unigram_tagger_regex_backoff.accuracy(nps_test_80)\n",
    "nps_train10 = combined_nps10_unigram_tagger_regex_backoff.accuracy(nps_test_90)\n",
    "\n",
    "accuracy_list = [nps_train90, nps_train80, nps_train50, nps_train20, nps_train10]\n",
    "trained_list = [\"nps_train90\", \"nps_train80\", \"nps_train50\", \"nps_train20\", \"nps_train10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4665703f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD5CAYAAAA9SqL2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAh+ElEQVR4nO3deXxU9d328c93Jiv7kgBKwLDvAhoIqIgVtMhDRVpr2SrKVnu7a29v6qN9rL2ltQtWW6wKCi4oUmqrtVjEjVpkC7IJCrJVFoEAiuwhye/5Yw46xgESMsmZ5Xq/Xnkxc+bMmSs/Qi7OMvMz5xwiIiJlBfwOICIisUkFISIiEakgREQkIhWEiIhEpIIQEZGIUvwOUFZWVpbLzc31O4aISFxZtmzZHudcdjS3GXMFkZubS0FBgd8xRETiipn9J9rb1CEmERGJSAUhIiIRqSBERCQiFYSIiESkghARkYhUECIiEpEKQkREIlJBJCHnHBt2H+SVlTvYe/CY33FEJEbF3BvlJPqcc2wsPMSiTXu9r33s8YqhbmYqP72iPdfkNSMQMJ+TikgsUUEkoFMVQpM6GVzUuiG9WjbknIY1eWjeeia8tJq/vL+NB4Z0oW3j2j6nF5FYoYJIAKFCOMjCTftYtGkvi8sUQp82WfRq2YBeLRvSvEENzL7aU8hv0YvZy7Yx8bUPGfjwu4y/uCU3X9qGzLSgX9+OiMSIchWEmQ0AHgaCwFTn3K/KPN4ceBqo560zwTk3x3vsXOBxoA5QCvRwzh2N1jeQjL5ZCHvZc7AIOH0hlBUIGNf0aEa/Do2YOOcjHn1nI39ftYNfDO7MJe0aVde3JCIxyE43J7WZBYH1wGXANmApMMw5tzZsnSeA5c65P5lZR2COcy7XzFKA94EfOudWmllD4HPnXMnJXi8vL8/pw/q+7sRJ5UWb9rJo876vFcJZdTPo1bJhuQvhdBZu3Mv//dtqNhUeYtC5Z/GzQR1pVCcjWt+KiFQRM1vmnMuL5jbLswfRE9jgnNvkhZgJDAbWhq3jCO0hANQFdni3LwdWOedWAjjn9kYjdKL7WiFs2sfizV8vhIvbZHul0JBmDTIrVQhl9W7VkNdu7cNj72xi8jsbmL+ukLuuaM+Ins11ElskyZSnIJoCW8PubwPyy6xzH/C6md0M1AT6e8vbAs7M5gLZwEzn3K/LvoCZjQfGAzRv3rwi+RNC2UJYtGkvew9VTyFEkp4S5Nb+bbiy29nc87fV3Pu3D/jLsm1MHNKFjmfXOf0GRCQhROsk9TBgunPud2bWG3jWzDp7278I6AEcBt70doPeDH+yc+4J4AkIHWKKUqaY5Zzj4y8LIXRS+UQhnF03g75tq7cQTqZFVk2eG5PPyyt28ItX1/KdP/6b0Rfmclv/ttRM1/UNIomuPP/KtwPNwu7neMvCjQEGADjnFppZBpBFaG/jX865PQBmNgc4D3iTJHLaQmgXKoTeLRuSU9+/QojEzLiqe1MuaZfNg//8iCnvbmbO6p38/MpO9O/Y2O94IlKFylMQS4E2ZtaCUDEMBYaXWecToB8w3cw6ABlAITAXuMvMagBFQF/goShlj1mlpV8VwuLN8VUIJ1OvRhq//O65fO+8HO7+62rGPlPAtzs15r4rO3FW3Uy/44lIFTjtVUwAZjYQ+D2hS1ifcs49YGb3AwXOuVe8K5emALUInbC+yzn3uvfckcBPveVznHN3neq14vEqpvBCCJXCPvZ5hdC0Xib53hVG8VQIp1JUXMrUf2/ikTc/JmjGnZe3Y9QFuQR1ElvEN1VxFVO5CqI6xUNBnK4Qwi87bdaghs9pq87WfYe5528fMH99IZ2b1mHikC6cm1PP71giSUkF4ZPSUsf63QdYtPGry04/O3wcSK5CiMQ5xz9Wf8rP/76WvQePcW3vXO68vC21M1L9jiaSVPx6H0TSOVUh5NTP5NL2jZO2EMoyMwadezYXt83mt3PX8fTCLbz2wafc951ODOjcJO4Pp4kkM+1BECqEdbsOfHmFUdlCOHHJaX6LBklfCKezYuvn3P3SatZ++gWXtm/Ez6/spDETqQY6xBQl4YVw4hzC5yqEqCkuKWX6e1uYNG89zsFt/dsw+qIWpAY1/YhIVVFBnKFTFUKzBpn0auEVQssG5NRXIUTL9s+PcN8ra5i3dhftm9TmgSFdOP+c+n7HEklIKohyKi11fLTzq0JYskWF4Ke5a3Zy3ytr2PnFUYb3bM5dA9pTN1MnsUWiSSepT2H3gaO8uvLTiIVwWYfGKgQffbtTEy5sncVD89YzbcFm5q7Zxb2DOnBl17N1ElskhiXMHsQH2/cz6A//pnmDGl9eYZTfsiFN6+ldvrHkg+37ufuvq1m1bT992mTxv1d15pyGNf2OJRL3dIjpFEpKHTu/OKpCiAMlpY7nFv2H38xdx/GSUm6+tDXjL25FWopOYoucqaooiIT5FxkMmMohTgQDxqgLcnnzzr7079CY376+noGPvMviTZouRCSWJExBSPxpXCeDySPOY9p1PTh6vIQfPLGIu2av5DPvY0tExF8qCPHdt9o3Yt7tfbmhbyteen87/SbNZ/aybcTa4U+RZKOCkJiQmRZkwhXtefWWi8htWIOf/Hklw6YsYmPhQb+jiSQtFYTElPZN6jD7hguYOKQLa3d8wRW/f5dJ89Zz9HiJ39FEko4KQmJOIGAMz2/Om3dewhVdmvDImx9zxcPvsmDDHr+jiSQVFYTErOza6Tw8tDvPjumJc44RUxdz+4sr2HPwmN/RRJKCCkJiXp822fzztou55dLWvLpqB/1+N5+ZSz6htFQnsUWqkgpC4kJGapA7Lm/Ha7f2oV2T2kx4aTXXPL6Q9bsO+B1NJGGpICSutG5UmxfH9+I3V5/LxsKDDHz4XR7850ccKdJJbJFoU0FI3DEzvp/XjDfvvISrujflT+9s5PLfz+eddbv9jiaSUFQQErca1Ezjt9/vyszxvUgLBrhu2lJufP59dn9x1O9oIglBBSFxr1fLhsy5tQ93XtaWeWt30e9383l24RZKdBJbpFJUEJIQ0lOC3NyvDXNvu5iuzepx78tr+O6f3mPNjv1+RxOJWyoISSgtsmry7JiePDy0G9s/O8yVf1zAA/9Yy6FjxX5HE4k7KghJOGbG4G5NefOOS7gmrxlT3t3MZZPmM2/tLr+jicQVFYQkrLo1Uvnld7vwlx/3pnZGKuOeKWD8MwXs+PyI39FE4oIKQhLe+ec04NVbLuJ/BrTnXx8Xctmk+Tz5780Ul5T6HU0kpqkgJCmkBgP8+JJWzLu9Lz1bNOAXr65l8OQFrNr2ud/RRGKWCkKSSrMGNXjquh48OuI8Cg8c46rJC7jvlTUcOHrc72giMUcFIUnHzBjY5SzeuLMvP+x1Dk8v3EL/SfOZs/pTzWInEkYFIUmrTkYqPx/cmb/914Vk1Urnv2a8z+jpS9m677Df0URiggpCkl7XZvV4+cYLuXdQRxZv3sdlD83nsfkbOa6T2JLkVBAiQEowwJiLWvDGHX25uE02v3rtI77zh3+z7D+f+R1NxDcqCJEwZ9fL5Ilr85hybR5fHDnO1Y+9x91/Xc3+wzqJLcmnXAVhZgPMbJ2ZbTCzCREeb25mb5vZcjNbZWYDIzx+0Mx+Eq3gIlXpso6NmXdHX8Zc2IIXl26l36R3eHnFdp3ElqRy2oIwsyAwGbgC6AgMM7OOZVa7B5jlnOsODAUeLfP4JOC1yscVqT4101O4Z1BHXrnpQprWr8GtM1dw1+xV+pRYSRrl2YPoCWxwzm1yzhUBM4HBZdZxQB3vdl1gx4kHzOwqYDOwptJpRXzQ6ey6vPTjC7jl0tb8edk2bnlhOUXFOoEtiS+lHOs0BbaG3d8G5JdZ5z7gdTO7GagJ9Acws1rA/wCXATq8JHErGDDuuLwddTJT+d9/fMiR4yU8OuI8MlKDfkcTqTLROkk9DJjunMsBBgLPmlmAUHE85Jw7eKonm9l4Mysws4LCwsIoRRKJvrF9WjJxSBfeXreb66ct1ceIS0IrT0FsB5qF3c/xloUbA8wCcM4tBDKALEJ7Gr82sy3AbcDdZnZT2Rdwzj3hnMtzzuVlZ2dX9HsQqVbD85vz0DXdWLJlHyOfXKwrnCRhlacglgJtzKyFmaUROgn9Spl1PgH6AZhZB0IFUeic6+Ocy3XO5QK/ByY65/4YrfAifrmqe1MeHXEea7Z/wbApi9hz8JjfkUSi7rQF4ZwrBm4C5gIfErpaaY2Z3W9mV3qr3QmMM7OVwAvAdU7XA0qC+3anJkwdlcemPQf5weML2bn/qN+RRKLKYu33eF5enisoKPA7hki5Ldm8j9HTl1K/ZirPj+1FswY1/I4kScjMljnn8qK5Tb2TWqSSerZowPPj8jlwtJirH3uPDbsP+B1JJCpUECJRcG5OPV4c35tSB9c8vog1O/b7HUmk0lQQIlHSrkltZv2oNxkpAYY9sUgf9CdxTwUhEkUtsmry5x9fQIOaafzwycW8t2GP35FEzpgKQiTKmtbLZNYNvWlWvwbXTV/KWx/t8juSyBlRQYhUgUa1M5g5vhftm9Rm/DPLeHXVjtM/SSTGqCBEqkj9mmnMGJvPec3rc8sLy5lVsPX0TxKJISoIkSpUOyOVp0f35MLWWdw1exXTF2z2O5JIuakgRKpYZlqQqaPy+Hanxtz397VMfnuD35FEykUFIVIN0lOCTB5+HkO6N+U3c9fx639+pNnpJOaVZz4IEYmClGCA332/K5lpQR59ZyOHi0r42aCOBALmdzSRiFQQItUoEDAeuKozNdOCTHl3MwePFfPg984lqJKQGKSCEKlmZsbdAztQKz2Vh95Yz5GiEh76QTfSUnTEV2KLCkLEB2bGrf3bUCMtyANzNIWpxCb9l0XER+Mu1hSmErtUECI+0xSmEqtUECIx4KruTZk8XFOYSmxRQYjEiAGdNYWpxBYVhEgMubhtNs+MzmfXF8f4/uPvsXXfYb8jSRJTQYjEGE1hKrFCBSESg05MYVpSqilMxT8qCJEY1a5Jbf58g6YwFf+oIERimKYwFT+pIERinKYwFb+oIETigKYwFT+oIETihKYwleqmghCJI5rCVKqTCkIkzmgKU6kuKgiROHRiCtOrup2tKUylymg+CJE4lRIMMOmabtRIT9EUplIlVBAicUxTmEpVUkGIxLkTU5jWTE/h9298rClMJWpUECIJwMy4rX9baqalaApTiRr9F0MkgWgKU4kmFYRIgtEUphIt5SoIMxtgZuvMbIOZTYjweHMze9vMlpvZKjMb6C2/zMyWmdlq789Lo/0NiMg3lZ3CdK+mMJUzcNqCMLMgMBm4AugIDDOzjmVWuweY5ZzrDgwFHvWW7wG+45zrAowCno1WcBE5tfApTK/RFKZyBsqzB9ET2OCc2+ScKwJmAoPLrOOAOt7tusAOAOfccufciU8VWwNkmll65WOLSHloClOpjPIURFMg/FPBtnnLwt0HjDSzbcAc4OYI2/ke8L5z7hv7umY23swKzKygsLCwXMFFpHx6tmjAjLGawlQqLlonqYcB051zOcBA4Fkz+3LbZtYJeBD4UaQnO+eecM7lOefysrOzoxRJRE7o2kxTmErFlacgtgPNwu7neMvCjQFmATjnFgIZQBaAmeUAfwWudc5trGxgETkzmsJUKqo8BbEUaGNmLcwsjdBJ6FfKrPMJ0A/AzDoQKohCM6sH/AOY4JxbELXUInJGNIWpVMRpC8I5VwzcBMwFPiR0tdIaM7vfzK70VrsTGGdmK4EXgOtc6KMlbwJaAz8zsxXeV6Mq+U5EpFya1stk1o80hamcnsXaRwTn5eW5goICv2OIJLzPDhUxatoS1u74gt8P7cagc8/2O5JUgpktc87lRXObeie1SJLSFKZyOioIkSSmKUzlVFQQIklOU5jKyaggRERTmEpEmg9CRABNYSrfpIIQkS9pClMJp4IQka/RFKZyggpCRL5BU5gK6CS1iJzCuItb8sCQzprCNEmpIETklEbkn8Oka7pqCtMkpIIQkdMa0j1HU5gmIRWEiJTLgM5NmKIpTJOKCkJEyq2vpjBNKioIEakQTWGaPFQQIlJhmsI0OaggROSMaArTxKeCEJEzpilME5sKQkQq5cQUpjn1MzWFaYJRQYhIpTWqk8GL43vTvkltxj+zjFdX7fA7kkSBCkJEouLEFKbdm9fTFKYJQgUhIlFTdgrTZxdu8TuSVIIKQkSiqkZaClNH5dG/Q2PufXkNLyz5xO9IcoZUECISdekpQSaP6M4l7bK5+6+rmb1sm9+R5AyoIESkSqSnBHls5Plc2CqL/569kpdXbPc7klSQCkJEqkxGapAp1+aR36IBd8xayZzVn/odSSpABSEiVSozLciTo3rQvVno6qbX1+z0O5KUkwpCRKpczfQUpl3fg85N63Lj8+/z9ke7/Y4k5aCCEJFqceIS2HZNavOj55bx7seFfkeS01BBiEi1qZuZynNj8mmZVZOxTxfw3kZ9dlMsU0GISLWqVyP0juvmDWowZnoBS7fs8zuSnIQKQkSqXcNa6cwYl89Z9TK4ftpS3v9EHxUei1QQIuKLRrUzeH5sLxrWSmPUU0tYvU2TDsUaFYSI+KZJ3QyeH9eLupmpjHxyMWt3fOF3JAmjghARXzWtl8kL43pRMy3IyCcXs26n5riOFeUqCDMbYGbrzGyDmU2I8HhzM3vbzJab2SozGxj22E+9560zs29HM7yIJIZmDWrw/LhepASMEVMXs2H3Qb8jCeUoCDMLApOBK4COwDAz61hmtXuAWc657sBQ4FHvuR29+52AAcCj3vZERL4mN6smz4/rBcDwKYvYvOeQz4mkPHsQPYENzrlNzrkiYCYwuMw6Dqjj3a4LnJhOajAw0zl3zDm3GdjgbU9E5BtaN6rFjLH5FJc6hk9ZxNZ9h/2OlNTKUxBNgfCpobZ5y8LdB4w0s23AHODmCjxXRORL7ZrU5rkx+RwuKmHYlEVs//yI35GSVrROUg8DpjvncoCBwLNmVu5tm9l4Mysws4LCQr39XiTZdTy7Ds+NyWf/keMMn7KInfuP+h0pKZXnl/h2oFnY/RxvWbgxwCwA59xCIAPIKudzcc494ZzLc87lZWdnlz+9iCSsLjl1eWZ0T/YeLGL41EXsPqCSqG7lKYilQBsza2FmaYROOr9SZp1PgH4AZtaBUEEUeusNNbN0M2sBtAGWRCu8iCS27s3rM+36Huzcf5QRUxaz9+AxvyMlldMWhHOuGLgJmAt8SOhqpTVmdr+ZXemtdicwzsxWAi8A17mQNYT2LNYC/wRudM6VVMU3IiKJqUduA54c1YOtnx1mxNTFfHaoyO9IScOcc35n+Jq8vDxXUFDgdwwRiTHvflzImKcLaNu4FjPGht59LV8xs2XOubxoblPvpBaRuNCnTTaPjzyfdTsPcO1TSzhw9LjfkRKeCkJE4sa32jdi8vDzWLN9P9dPW8qhY8V+R0poKggRiSuXd2rCI8O6s3zr54yevpQjRTqtWVVUECISdwZ2OYtJ13Rl6ZZ9jHumgKPHVRJVQQUhInFpcLem/PrqrizYuIcbnlvGsWKVRLSpIEQkbl19fg4Th3ThnXWF3DhjOUXFpX5HSigqCBGJa8N6NucXgzvxxoe7uHXmcopLVBLRooIQkbj3w9653DuoI699sJPbZ62kpDS23t8Vr1L8DiAiEg1jLmrB8ZJSfvXaR6QGjd9e3ZVAwPyOFddUECKSMG7o24qi4lImzVtPWjDAxCFdVBKVoIIQkYRyS782HC8p5Q9vbSAlaPxicGfMVBJnQgUhIgnnjsvaUlRSyuPzN5EaDPCzQR1VEmdABSEiCcfMmDCgPUXFpUxbsIW0lAATBrRXSVSQCkJEEpKZ8bNBHTnu7UmkBwPccXk7v2PFFRWEiCQsM+P+KztTXOJ45K0NpAQD3NKvjd+x4oYKQkQSWiBgTBzShaIS7+qmlAA39G3ld6y4oIIQkYQXCBi/uborxSXOe59EgDEXtfA7VsxTQYhIUggGjEnXdOV4SSm/eHUtaUHjh71z/Y4V0/RRGyKSNFKCAR4e2p3+HRpz78trmLnkE78jxTQVhIgklbSUAJNHdOeSdtn89K+rmb1sm9+RYpYKQkSSTnpKkMdGns+FrbK4a/ZKXl6x3e9IMUkFISJJKSM1yJRr8+iR24A7Zq1kzupP/Y4Uc1QQIpK0MtOCPHVdD7o3q8ctLyzn9TU7/Y4UU1QQIpLUaqanMO36HnRuWpcbn3+ftz/a7XekmKGCEJGkVzsjladH96Rdk9r86LllvPtxod+RYoIKQkQEqJuZyrOj82mZVZNxzxSwcONevyP5TgUhIuKpXzONGWPzaVa/BmOeXsrSLfv8juQrFYSISJiGtdKZMS6fJnUyuH7aUpZ/8pnfkXyjghARKaNR7QyeH9eLhrXSuPapJazett/vSL5QQYiIRNCkbqgk6mamMvLJxazd8YXfkaqdCkJE5CSa1svkhXG9qJEWZOSTi1m/64DfkaqVCkJE5BSaNajBC+N6kRIwhk9ZzIbdB/2OVG1UECIip5GbVZPnx/UCHMOnLGLLnkN+R6oWKggRkXJo3agWM8b24nhJKcOnLGLrvsN+R6pyKggRkXJq16Q2z43N51BRCcOmLGL750f8jlSlylUQZjbAzNaZ2QYzmxDh8YfMbIX3td7MPg977NdmtsbMPjSzR8zMophfRKRadTq7Ls+NyWf/keMMn7KInfuP+h2pypy2IMwsCEwGrgA6AsPMrGP4Os65251z3Zxz3YA/AC95z70AuBA4F+gM9AD6RvMbEBGpbl1y6vL06J7sOXCM4VMXsftAYpZEefYgegIbnHObnHNFwExg8CnWHwa84N12QAaQBqQDqcCuM48rIhIbzmten+mje/Lp50cZOXUxew8e8ztS1JWnIJoCW8Pub/OWfYOZnQO0AN4CcM4tBN4GPvW+5jrnPozwvPFmVmBmBYWF+hRFEYkPPXIb8OR1efxn72FGTF3MZ4eK/I4UVdE+ST0UmO2cKwEws9ZAByCHUKlcamZ9yj7JOfeEcy7POZeXnZ0d5UgiIlXnglZZTLk2j017DvHDpxaz/8hxvyNFTXkKYjvQLOx+jrcskqF8dXgJYAiwyDl30Dl3EHgN6H0mQUVEYtXFbbN5fOT5rNt5gFFPLeHA0cQoifIUxFKgjZm1MLM0QiXwStmVzKw9UB9YGLb4E6CvmaWYWSqhE9TfOMQkIhLvvtW+EZOHn8cH2/dz/bSlHDpW7HekSjttQTjnioGbgLmEfrnPcs6tMbP7zezKsFWHAjOdcy5s2WxgI7AaWAmsdM79PWrpRURiyOWdmvDIsO4s3/o5Y55eypGiEr8jVYp9/fe5//Ly8lxBQYHfMUREztjLK7Zz24sruLBVFlNH5ZGRGqzy1zSzZc65vGhuU++kFhGJssHdmvKbq7uyYOMebnhuGceK43NPQgUhIlIFrj4/h4lDuvDOukJunLGcouJSvyNVmApCRKSKDOvZnPsHd+KND3dx68zlFJfEV0moIEREqtC1vXO55/904LUPdnLHrJWUlMbWed9TSfE7gIhIohvbpyXHSxwP/vMjUoLGb6/uSiAQ+59bqoIQEakGP76kFcdLSpk0bz1pwQATh3SJ+ZJQQYiIVJNb+rWhqLiUP769gdRggPsHdyKWZ0BQQYiIVKM7L2/L8ZJSHv/XJlKDAe4d1CFmS0IFISJSjcyMCVe0p6iklKcWbCY1xZgwoH1MloQKQkSkmpkZPxvUMbQnMX8T6cEAd1zezu9Y36CCEBHxgZlx/5WdOV7seOSt0DmJm/u18TvW16ggRER8EggYv/xuF46XlPK7eetJTQlwQ99Wfsf6kgpCRMRHgYDxm+935Xipi7l3WqsgRER8FgwYD/+gW8y9L0IftSEiEgNirRxABSEiIiehghARkYhUECIiEpEKQkREIlJBiIhIRCoIERGJSAUhIiIRmXOxNf2dmRUC/6nEJrKAPVGKkww0XhWj8aoYjVfFVGa8znHOZUczTMwVRGWZWYFzLs/vHPFC41UxGq+K0XhVTKyNlw4xiYhIRCoIERGJKBEL4gm/A8QZjVfFaLwqRuNVMTE1Xgl3DkJERKIjEfcgREQkClQQIiISkQpCREQiisuCMLNcMxt+hs99rxzrXGpm75vZB2b2tJmleMvNzB4xsw1mtsrMzjuTDNWtGsarnzdeK8zs32bW2luebmYveuO12MxyzyRDdauG8ZpuZpu98VphZt285fr5irzODDNb5/17fMrMUr3lGq/I69zkjYkzs6yw5RUer7gsCCAXiDjAJ36Zn4xz7oJTPW5mAeBpYKhzrjOhd3WP8h6+AmjjfY0H/lSh1P7JpYrGy/MnYIRzrhvwPHCPt3wM8JlzrjXwEPBgOfP6LZeqHS+A/3bOdfO+VnjL9PMV2QygPdAFyATGess1XpEtAPrzzU+kqPh4Oeeq7IvQQHwITAHWAK8T+gt+B3gYWAF8APT01u/rLVsBLAdqn2S7i4D93nq3A9cBrwBvAfOBWsCbwPvAamBw2HMPen9e4uWYDXxE6IfQgGxgY9j6fYA53u3HgWFhj60Dzkrm8Qobh3zv9k+Bid7tuUBv73YKoY8QMI0X04GrI7yufr4ijFeZ17odeEDjdfrxArYAWZX5+YrKQJ5mgIuBbt79WcBI7xub4i27GPjAu/134ELvdi0g5STbvQR4Nez+dcA2oIF3PwWo493OAjbw1T/O8AHeD+QQ2pNaCFxEqCT+A+R56z0MrPZuvwpcFPa6b55YL1nHy3usD7DX2+basG19AOSEve7G8B/YJB6v6YT+ca4itGeVrp+vk49X2HZTCf3S7KPxKtd4beHrBVHh8aqOQ0yb3Ve70MsIDTrACwDOuX8BdcysHqFdo0lmdgtQzzlXXIHXmeec2+fdNmCima0C3gCaAo0jPGeJc26bc66UUJvnutDIDQUeMrMlwAGgpAI5Kiuuxstbfjsw0DmXA0wDJlUgR2XF43j9lNAhkx5AA+B/KpCjsuJxvE54FPiXc+7dCuSorHger0qrjoI4Fna7hFA7ApR9h55zzv2K0PHFTGCBmbWvwOscCrs9gtChovNd6Lj4LiCjvNmccwudc32ccz2BfwHrvXW2A83CnpPjLYumuBovM8sGujrnFnvLXwROHCf9cry8Y6t1Ce1pRFNcjZcX5FMXcoxQofb01tHPV+RsmNn/87ZxR9g6Gq/I2U6mwuPl50nqHwCY2UXAfufcfjNr5Zxb7Zx7EFhK6H9ZkRwAap9i23WB3c6542b2LeCcigQzs0ben+mE/nf3mPfQK8C13tUAvbzcn1Zk25UQq+P1GVDXzNp69y8jdNwWQuM1yrt9NfCWt4dWHWJ1vDCzs7w/DbiK0KE40M9XRGY2Fvg2oePnpWEPabwqpsLjdbrGqUpHzWw5oeOKo71lt3kDUkropNBrJ3nuKqDEzFYSOp77WZnHZwB/N7PVQAGhkzgV8d9mNohQgf7JOfeWt3wOMJDQMcHDwPUV3G5lxOR4OeeKzWwc8BczK/W2fSLfk8CzZrYB2Efo0F11icnxOvF8b8/LCB0auMFbrp+vyB4jdF5wYahTeck5dz8ar4i8Q1x3AU2AVWY2xzk3ljMYL18+i8nM3gF+4pwrqPYXj0Mar4rReFWMxqtikmm84vV9ECIiUsVi+tNczawL8GyZxcecc/l+5Il1Gq+K0XhVjMarYhJhvGK6IERExD86xCQiIhGpIEREJCIVhIiIRKSCEBGRiP4/BO2xTjEBi8AAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(trained_list, accuracy_list)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded44a1c",
   "metadata": {},
   "source": [
    "From the graph we can see that the optimal train/test ratio is 80/20."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675f7988",
   "metadata": {},
   "source": [
    "## 3 – Tagging with probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd15775",
   "metadata": {},
   "source": [
    "**Hidden Makrov Models (HMMs) can be used to solve Part-of-Speech (POS) tagging. Use HMMs to\n",
    "calculate probabilities for words and tags, using the appended code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f8c96",
   "metadata": {},
   "source": [
    "**(a) Implement the missing pieces of the function task3a() found in the appended code. Also found on\n",
    "the next page for reference.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "# see https://nltk.readthedocs.io/en/latest/api/nltk.html\n",
    "# define distinguishable start/end tuples of tag/word\n",
    "# used to mark sentences\n",
    "START = (\"START\", \"START\")\n",
    "END = (\"END\", \"END\")\n",
    "\n",
    "def get_tags(corpus):\n",
    "    tags_words = []\n",
    "    for sent in corpus.tagged_sents():\n",
    "        tags_words.append(START)\n",
    "        # shorten tags to 2 characters each for simplicity\n",
    "        tags_words.extend([(tag[:2], word) for (word, tag) in sent])\n",
    "        tags_words.append(END)\n",
    "\n",
    "    return tags_words\n",
    "\n",
    "def probDist(corpus, probability_distribution, tag_observation_fn):\n",
    "    tag_words = get_tags(corpus)\n",
    "    tags = [tag for (tag, _) in tag_words]\n",
    "    # conditional frequency distribution over tag/word\n",
    "    cfd_tagwords = nltk.ConditionalFreqDist(tag_words)\n",
    "    cpd_tagwords = nltk.ConditionalProbDist(cfd_tagwords, probability_distribution)\n",
    "    # conditional frequency distribution of observations:\n",
    "    cfd_tags = nltk.ConditionalFreqDist(tag_observation_fn(tags))\n",
    "    cpd_tags = nltk.ConditionalProbDist(cfd_tags, probability_distribution)\n",
    "    \n",
    "    return cpd_tagwords, cpd_tags\n",
    "\n",
    "def task3a():\n",
    "    corpus = brown\n",
    "    # maximum likelihood estimate to create a probability distribution \n",
    "    probability_distribution = nltk.probability.MLEProbDist\n",
    "    tag_observation_fn = nltk.bigrams\n",
    "    return probDist(corpus, probability_distribution, tag_observation_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d6b7ce",
   "metadata": {},
   "source": [
    "**(b) Print the probablity of:\n",
    "• a verb (VB) being “run”\n",
    "• a preposition (PP) being followed by a verb\n",
    "A template is found in the code under task3b()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "709c69ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob. of a Verb(VB) being 'run' is 0.1329%\n",
      "Prob. of a Preposition(PP) being followed by a Verb(VB) is 25.1591%\n"
     ]
    }
   ],
   "source": [
    "def prettify(prob):\n",
    "    return \"{}%\".format(round(prob * 100, 4))\n",
    "    \n",
    "def task3b():\n",
    "    tagwords, tags = task3a()\n",
    "    prob_verb_is_run =  tagwords['VB'].prob('run')\n",
    "    prob_v_follows_p = tags['PP'].prob('VB')\n",
    "    print(\"Prob. of a Verb(VB) being 'run' is\", prettify(prob_verb_is_run))\n",
    "    print(\"Prob. of a Preposition(PP) being followed by a Verb(VB) is\", prettify(prob_v_follows_p))\n",
    "    \n",
    "task3b()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c55a3fa",
   "metadata": {},
   "source": [
    "**(c) Print the 10 most common words for each of the tags NN, VB, JJ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1cc43dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpd_tagwords, cpd_tags = task3a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f210a3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN [('time', 1555), ('man', 1148), ('Af', 994), ('years', 942), ('way', 883), ('people', 809), ('men', 736), ('world', 684), ('life', 676), ('year', 647)]\n",
      "VB [('said', 1943), ('made', 1119), ('make', 765), ('see', 727), ('get', 719), ('know', 676), ('came', 621), ('used', 610), ('go', 604), ('come', 589)]\n",
      "JJ [('new', 1060), ('such', 903), ('own', 750), ('good', 693), ('great', 592), ('New', 575), ('old', 568), ('American', 535), ('small', 517), ('long', 515)]\n"
     ]
    }
   ],
   "source": [
    "tags = ['NN','VB','JJ']\n",
    "\n",
    "for tag in tags:\n",
    "    print(tag, cpd_tagwords[tag].freqdist().most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb282c8",
   "metadata": {},
   "source": [
    "**(d) Print the probability of the tag sequence PP VB VB DT NN for the sentence “I can code some\n",
    "code”**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "85a88c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.646546474434433e-22\n"
     ]
    }
   ],
   "source": [
    "prob = cpd_tagwords[\"PP\"].prob(\"I\") * cpd_tags[\"PP\"].prob(\"VB\") * cpd_tagwords[\"VB\"].prob(\"can\") * cpd_tags[\"VB\"].prob(\"VB\")* cpd_tagwords[\"VB\"].prob(\"code\")  * cpd_tags[\"VB\"].prob(\"DT\") * cpd_tagwords[\"DT\"].prob(\"some\") * cpd_tags[\"DT\"].prob(\"NN\") * cpd_tagwords[\"NN\"].prob(\"code\") * cpd_tags[\"VB\"].prob(\"NN\")\n",
    "\n",
    "print(prob)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
